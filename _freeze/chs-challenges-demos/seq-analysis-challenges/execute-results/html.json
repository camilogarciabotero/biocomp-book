{
  "hash": "1d20a04e1d276cd1c4c75ec9b09b426a",
  "result": {
    "markdown": "---\nexecute: \n  freeze: auto\nfilters:\n  - code-filename\n  - nutshell\n  - lightbox\n---\n\n\n# Seq. Analysis challenges {.unnumbered}\n\n## Counting features\n\nThe first step in this journey is to download a bunch of sequences programmatically. To do so, we will use the program [ncbi-genome-download](https://github.com/kblin/ncbi-genome-download).\n\nYou could inspect all the options it provides, now we will set our command as the following:\n\n```{.zsh}\n#| echo: true\n#| eval: false\nngd --genera \"Bacillus subtilis\"\\\n    -s refseq\\\n    -l complete\\\n    -o Data\\\n    --flat-output\\\n    --format features\\\n    -n bacteria\\\n    | head -n 5\n```\nTo this date this command will search for 216 complete genome information of *Bacillus subtilis* strains and download the **feature-table** file compressed. So the next step is to decompress all of them:\n\n```{.zsh}\nfor i in *.txt; do\n  gzip -d $i\ndone\n```\nNow the the feature-table file is a is a long table containing each of the features annotated in the genome see the top of a file:\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-1_39a4c5c93d185aa07675f288a7238366'}\n\n```{.bash .cell-code}\nhead -n 5 data/features/GCF_000009045.1_ASM904v1_feature_table.txt\n```\n\n\n::: {.cell-output .cell-output-stdout}\n```\n# feature\tclass\tassembly\tassembly_unit\tseq_type\tchromosome\tgenomic_accession\tstart\tend\tstrand\tproduct_accession\tnon-redundant_refseq\trelated_accession\tname\tsymbol\tGeneID\tlocus_tag\tfeature_interval_length\tproduct_length\tattributes\ngene\tprotein_coding\tGCF_000009045.1\tPrimary Assembly\tchromosome\t\tNC_000964.3\t410\t1750\t+\t\t\t\t\tdnaA\t939978\tBSU_00010\t1341\t\told_locus_tag=BSU00010\nCDS\twith_protein\tGCF_000009045.1\tPrimary Assembly\tchromosome\t\tNC_000964.3\t410\t1750\t+\tNP_387882.1\tWP_003242674.1\t\tchromosomal replication initiator informational ATPase\tdnaA\t939978\tBSU_00010\t1341\t446\t\ngene\tprotein_coding\tGCF_000009045.1\tPrimary Assembly\tchromosome\t\tNC_000964.3\t1939\t3075\t+\t\t\t\t\tdnaN\t939970\tBSU_00020\t1137\t\told_locus_tag=BSU00020\nCDS\twith_protein\tGCF_000009045.1\tPrimary Assembly\tchromosome\t\tNC_000964.3\t1939\t3075\t+\tNP_387883.1\tWP_003242509.1\t\tDNA polymerase III (beta subunit)\tdnaN\t939970\tBSU_00020\t1137\t378\t\n```\n:::\n:::\n\n\nThe question now is how to count the lines corresponding to `features` which is the first line. It contains six types of features (CDS, gene, rRNA, tRNA, tmRNA, ncRNA and misc_RNA). This task could be achieved by many ways, but a general approach to count lines is the way through it. Here there three approaches to follow:\n\n:::{.panel-tabset}\n\n### Bash\n\n```{.bash}\n#! usr/bin/bash\n\nexport features=\"id CDS gene ncRNA rRNA tmRNA tRNA\"\n\necho $features\n\n\nfor i in $(ls $1); do\n    values=$(awk '/CDS/{++cnt1} /gene/{++cnt2} /ncRNA/{++cnt3} /rRNA/{++cnt4} /tmRNA/{++cnt5} /tRNA/{++cnt} END {print cnt1, cnt2, cnt3, cnt4, cnt5, cnt6}' ${1}/${i});\n    id=$(egrep -o -m 1 \"GCF.{12}\" ${1}/${i})\n    echo \"$id $values\"\ndone\n```\n\n```\nid CDS gene ncRNA rRNA tmRNA tRNA\nGCF_000009045.1 4238 4578 4 76 2 \nGCF_000146565.1 3998 4120 6 64 2 \nGCF_000186745.1 4111 4247 6 78 2 \nGCF_000209795.2 4262 4400 6 76 2 \nGCF_000227465.1 4167 4314 6 76 2 \nGCF_000227485.1 3991 4128 6 76 2 \nGCF_000293765.1 4205 4343 6 76 2 \nGCF_000321395.1 4043 4179 6 76 2 \n```\n\n### R\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/file-reading_be304f303eec0377bba1594300a3be0d'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(fs)\n\nall_features <- dir_ls(\"data/features\") |> \n  map_df(read_tsv)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, see `problems()` for details\n```\n:::\n\n```{.r .cell-code}\nall_features |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 20\n  `# feature` class   assem…¹ assem…² seq_t…³ chrom…⁴ genom…⁵ start   end strand\n  <chr>       <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <dbl> <dbl> <chr> \n1 gene        protei… GCF_00… Primar… chromo… <NA>    NC_000…   410  1750 +     \n2 CDS         with_p… GCF_00… Primar… chromo… <NA>    NC_000…   410  1750 +     \n3 gene        protei… GCF_00… Primar… chromo… <NA>    NC_000…  1939  3075 +     \n4 CDS         with_p… GCF_00… Primar… chromo… <NA>    NC_000…  1939  3075 +     \n5 gene        protei… GCF_00… Primar… chromo… <NA>    NC_000…  3206  3421 +     \n6 CDS         with_p… GCF_00… Primar… chromo… <NA>    NC_000…  3206  3421 +     \n# … with 10 more variables: product_accession <chr>,\n#   `non-redundant_refseq` <chr>, related_accession <lgl>, name <chr>,\n#   symbol <chr>, GeneID <dbl>, locus_tag <chr>, feature_interval_length <dbl>,\n#   product_length <dbl>, attributes <chr>, and abbreviated variable names\n#   ¹​assembly, ²​assembly_unit, ³​seq_type, ⁴​chromosome, ⁵​genomic_accession\n```\n:::\n:::\n\n\nNow that we read all the files into the programming environment we can operate over them with different libraries.\n\n::: {.cell hash='seq-analysis-challenges_cache/html/data-processing_af369aad45c9056ba5920f982acb8686'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nall_features_grouped <- all_features |> \n  rename(feature = `# feature`) |> \n  select(assembly, feature) |> \n  group_by(assembly, feature) |>\n  count() |> \n  pivot_wider(names_from = feature, values_from = n)\n\nall_features_grouped\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n# Groups:   assembly [5]\n  assembly          CDS  gene misc_RNA ncRNA  rRNA  tRNA tmRNA\n  <chr>           <int> <int>    <int> <int> <int> <int> <int>\n1 GCF_000009045.1  4237  4536       93     2    30    86    NA\n2 GCF_000146565.1  3998  4104       NA     4    24    77     1\n3 GCF_000186745.1  4111  4230       NA     4    31    83     1\n4 GCF_000209795.2  4262  4384       NA     4    30    87     1\n5 GCF_000227465.1  4167  4294       NA     4    30    92     1\n```\n:::\n:::\n\nThe code lines operation are doing the following steps:\n\n1. Create a new dataset that will group by features per accession.\n2. Change the `# feature` name for simple `feature`.\n3. Select `feature` and `assembly`  columns.\n4. Group by these two columns, enabling grouping operations.\n5. Count the numbers of rows based on the applied grouping.\n6. Generate a wide dataset sending row names as columns.\n\n### Python\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-4_163e6a8d341e685b1dc3cb738f11bbe0'}\n\n```{.python .cell-code}\nimport glob\nimport pandas as pd\n\nfiles = glob.glob(\"data/features/*.txt\")\n\ndf = pd.concat((pd.read_csv(f, sep='\\t') for f in files))\n```\n:::\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-5_ccdfead72b0aebbb3284505beaeef7c9'}\n\n```{.python .cell-code}\ndf_renamed  = df.rename(columns={\"# feature\" : \"feature\"})\n```\n:::\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-6_1750129558b737dda4e69e3d9ebc9123'}\n\n```{.python .cell-code}\ndf_filtered = df_renamed.filter(items=[\"feature\", \"assembly\"])\n```\n:::\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-7_9cb2137cda3af10f17cee19d36889148'}\n\n```{.python .cell-code}\ndf_filtered.groupby([\"assembly\",\"feature\"])[\"feature\"].count()\n```\n:::\n\n\n:::\n\n## Sanger processing\n\nProcessing a Sanger `AB1` file is a very common task in bioinformatics, yet it is sometimes taked for grandted. Many graphical programs allow to process the sequences, yet the task are currently manual involving the trimming and reverse complement generation of the reverse reads (when pair-end sequencing). But, given the vast generation of data on the present, Sanger sequencing is used massively in parallel to generate huge amount of sequences. Therefore, processing \"by-hand\" becomes an unachivable task.\n\nSeveral programs have been developed to automate the processing of sequences, an open source library in `R` is `sangeranalyseR`.\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/libs-_c41d24571ea1284ff8a0780d2b58a6f8'}\n\n```{.r .cell-code}\nlibrary(sangeranalyseR)\n```\n:::\n\n\n### Processing a single gene `AB1` pair\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/single-sanger-processing_d88f416bbeb41366c3d49aa94753b74d'}\n\n```{.r .cell-code}\nrpoB <- SangerAlignment(\n  ABIF_Directory = \"data/sanger-seqs/rpoB/\",\n  REGEX_SuffixForward = \"rpoB_1_F.ab1\",\n  REGEX_SuffixReverse = \"rpoB_2_R.ab1\",\n  TrimmingMethod = \"M2\",\n  M2CutoffQualityScore = 33,\n  M2SlidingWindowSize = 5\n)\n\nrpoB\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSangerAlignment S4 instance\n           Input Source :  ABIF \n         Process Method :  REGEX \n         ABIF Directory :  data/sanger-seqs/rpoB/ \n   REGEX Suffix Forward :  rpoB_1_F.ab1 \n   REGEX Suffix Reverse :  rpoB_2_R.ab1 \n      Contigs Consensus :  TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTKGGGGGGGAGWACTTGAGGAGCCATCATCATGAGTGAACGTCTTGTGAAGGATGATGTTTATACATCTATCCATATTGAAGAATATGAATCAGAAGCACGTGATACGAAACTTGGACCTGAAGAAATCACTCGCGATATTCCAAACGTCGGTGAAGATGCGCTTCGCAATCTTGATGACCGCGGAATCATCCGTATTGGGGCAGAAGTAAAAGACGGAGATCTTCTTGTTGGTAAAGTAACGCCTAAAGGTGTAACCGAACTGACTGCTGAAGAACGCCTTCTTCACGCCATCTTTGGTGAAAAAGCCCGCGAGGTTCGTGATACTTCTCTTCGTGTGCCTCACGGCGGCGGCGGAATTATCCACGACGTTAAAGTCTTCAACCGTGAAGACGGAGACGAACTTCCTCCAGGTGTAAACCAGTTGGTACGCGTATATATCGTTCAGAAACGTAAGATTTCTGAAGGGGATAAAATGGCCGGTCGTCACGGTAACAAAGGTGTTATCTCTAAGATTCTTCCTGAAGAGGATATGCCTTACCTTCCTGACGGCACACCAATTGACATCATGCTTAACCCGCTGGGCGTACCATCACGTATGAACATCGGGCAGGTATTGGAGCTTCATATGGGTATGGCCGCTCGTTACCTCGGCATTCACATTGCGTCTCCTGTATTTGATGGAGCGCGAGAAGAGGATGTTTGGGAAACACTTGAAGAAGCCGGCATGTCTCGTGACGCCAAAACGGTGCTTTACGACGGACGTACTGGAGAGCCGTTTGATAACCGCGTGTCTGTCGGTATCATGTACATGATCAAACTGGCACACATGGTTGACGATAAACTTCATGCACGCTCTACAGGTCCTTACTCACTTGTTACGCAGCAGCCTCTTGGCGGTAAAGCGCAATTTGGCGGACAGCGTTTTGGTGAGATGGAGGTTTGGGCACTTGAAGCTTATGGTGCAGCTTACACTCTTCAAGAAATTCTGACTGTTAAGTCCGATAACGTGGTTGGACGTGTGAAACTACACCTCDSTGGCACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA \n```\n:::\n:::\n\n\n This produces an specialized `R` object holding different information from the Sanger processing steps. The entire object can be untangled in a thorough report displaying all the processing information and results using the \n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-10_b1cdb2b3de3304f3d2b2a168f93c5d63'}\n\n```{.r .cell-code}\ngenerateReport(rpoB, outputDir = \"data/sanger-processed/\")\n```\n:::\n\n\nAmong the data from the report ypu can see that the unaligned contigs ithere is something called a *Contig Consensus* which is the result of comparing the three consensus out of the **rpoB** genes of the three strains (302, 321, 455). Now, this is not really the sequence we need right away, however we need the separate consensus of each strain gene. To get theme `sangeranalyseR` provide a couple of tools.\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-11_2e4b9217e4b106bfcb8750c9498b9488'}\n\n```{.r .cell-code}\nwriteFasta(rpoB,\n  outputDir = \"data/sanger-processed/rpoB\",\n  selection = \"contigs_unalignment\",\n)\n```\n:::\n\n\nNow, in the case of the challenge we actually got several genes per strain which is common in molecular biology projects. How do we process all the batch of sequence all at a time? To do so we can split the sequences as folder per gene, then we can use some capabilites of are to iterate an map the sanger processing function to each folder and create a list of processing instances, let's try this out:\n\n### Processing a bulk of `AB1` pair\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/bulk-sanger-processing_66dd304d868bf38f8d7e9e6628536830'}\n\n```{.r .cell-code}\nlibrary(fs)\nlibrary(purrr)\n\ndirs <- fs::dir_ls(\"data/sanger-seqs/\")\n\n\nsanger_bulk <- function(dir) {\n  SangerAlignment(\n    ABIF_Directory = dir,\n    REGEX_SuffixForward = \"_1_F.ab1\",\n    REGEX_SuffixReverse = \"_2_R.ab1\",\n    M2CutoffQualityScore = 33,\n    M2SlidingWindowSize = 2\n  )\n}\n\ngenes <- dirs |> \n  map(sanger_bulk)\n\ngenes$\n\ngenes$`data/sanger-seqs/rpoB`\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n\n```{.r .cell-code}\ngenes$`data/sanger-seqs/spo0B`\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSangerAlignment S4 instance\n           Input Source :  ABIF \n         Process Method :  REGEX \n         ABIF Directory :  data/sanger-seqs/spo0B \n   REGEX Suffix Forward :  _1_F.ab1 \n   REGEX Suffix Reverse :  _2_R.ab1 \n      Contigs Consensus :  ATCAACCATGCCAGAAAAAAGGAACATGATATTTCTGGGAAAAACMATTGTTTTTCTAACAAAGCCTTACTCTGTTATAATTCATAATACACACTTATACAGACTCCTAAATAAGAAATTAAATGATTGGGAGTGCGAAAATGAAGGATGTTTCAAAAAATCAAGAAGAAAATATAAGCGACACGGCATTAACAAACGAACTGGTTCATCTGCTGGGCCATTCCCGGCATGATTGGATGAATAAGCTGCAGCTGATTAAAGGAAACTTAAGCTTGCAGAAGTATGACCGCGTCTTTGAAATGATTGAAGAAATGGTTATAGACGCGAAGCACGAATCAAAGCTCTCAAACCTAAAAACACCGCATTTGGCGTTTGATTTTCTTACATTTAATTGGAAAACCCATTATATGACGCTTGAATATGAAGTTCTTGGAGAAATTAAGGATTTGTCGGCTTATGATCAAAAGCTGGCAAAACTGATGAGAAAGCTGTTTCATCTGTTTGATCAAGCAGTCAGCAGAGAGAGTGAAAATCATTTAACGGTTTCGCTTCAAACAGATCATCCTGACAGACAGCTGATTCTGTACCTTGATTTTCACGGCGCCTTTGCCGATCCTTCTGCTTTTGATGATATTCGGCAGAATGGATACGAGGATGTGGATATCATGCGTTTTGAGATCACAAGCCACGAATGTCTGATTGTAAATAGGGTTGGTACTAGCGGTAGTTTTTAACGGTTTAGAACGGAGGACATTATGTTTGTAGATCAGGTCAAAGTATATGTAAAAGGCGGCG \n```\n:::\n:::\n\n\nFinally to generate reports we need to call the appropriate instance with the `$` to select the gene we are interested to inspect:\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-13_f8ef22f82b6133a31a92a326d9ad34cd'}\n\n```{.r .cell-code}\ngenerateReport(genes$`data/sanger-seqs/spo0B`, outputDir =\"data/sanger-processed/spo0B\")\n\n\nwriteFasta(\n genes$`data/sanger-seqs/rpoB`,\n outputDir = \"data/sanger-processed/rpoB\",\n selection = \"contigs_unalignment\"\n)\n```\n:::\n\n\nNow that we got the actual multifasta of the genes from all strain, the identification process using `blastn` from the command line will follow as:\n\n\n::: {.cell hash='seq-analysis-challenges_cache/html/unnamed-chunk-14_985792831eb01501d55d4c04bf1fb21e'}\n\n```{.bash .cell-code}\nblastn -db nr -query rpoB-unaligned-contigs.mfa -out rpoB-blastn-report.txt -remote\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}