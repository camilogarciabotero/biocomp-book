{
  "hash": "fb9c43eebc456ddef9c7fd1045f2e32d",
  "result": {
    "markdown": "---\nexecute:\n  freeze: true\n---\n\n\n## Counting features {.unnumbered}\n\n\nThe first  downloading from NCBI\n\nThe first step in this journey is to download a bunch of sequences programmatically. To do so, we will use the program [ncbi-genome-download](https://github.com/kblin/ncbi-genome-download).\n\nYou could inspect all the options it provides, now we will set our command as the following:\n\n```{.zsh}\n#| echo: true\n#| eval: false\nngd --genera \"Bacillus subtilis\"\\\n    -s refseq\\\n    -l complete\\\n    -o Data\\\n    --flat-output\\\n    --format features\\\n    -n bacteria\\\n    | head -n 10\n```\n\n\n```{.zsh}\ngzip -d *\n```\n\n. . .\n\nA file from the download is a long table that...\n\n\n:::{.panel-tabset}\n\n## Bash\n\n```{.bash}\n#! usr/bin/bash\n\nexport features=\"id CDS gene ncRNA rRNA tmRNA tRNA\"\n\necho $features\n\n\nfor i in $(ls $1); do\n    values=$(awk '/CDS/{++cnt1} /gene/{++cnt2} /ncRNA/{++cnt3} /rRNA/{++cnt4} /tmRNA/{++cnt5} /tRNA/{++cnt} END {print cnt1, cnt2, cnt3, cnt4, cnt5, cnt6}' ${1}/${i});\n    id=$(egrep -o -m 1 \"GCF.{12}\" ${1}/${i})\n    echo \"$id $values\"\ndone\n```\n\n## R\n\n\n::: {.cell warclass-warning='false' warattr-warning='false' hash='demo-genome-searching_cache/html/file-reading_381adbdecf8961531c5ef0858911dcf8'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(fs)\n\nall_features <- dir_ls(\"Data/\") |> \n  map_df(read_tsv)\n\nall_features |> \n  head()\n```\n:::\n\n\nNow that we read all the files into the programming environment we can operate over them with different libraries.\n\n::: {.cell hash='demo-genome-searching_cache/html/data-processing_39276b8b305ef0c61a9189449dbce4fb'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nall_features_grouped <- all_features |> \n  rename(feature = `# feature`) |> \n  select(assembly, feature) |> \n  group_by(assembly, feature) |>  operations\n  count() |> \n  pivot_wider(names_from = feature, values_from = n)\n```\n:::\n\n\n1. Create a new dataset that will group by features per accession.\n2. Change the `# feature` name for simple `feature`.\n3. Select `feature` and `assembly`  columns.\n4. Group by these two columns, enabling grouping operations.\n5. Count the numbers of rows based on the applied grouping.\n6. Generate a wide dataset sending row names as columns.\n\n## Python\n\n\n::: {.cell hash='demo-genome-searching_cache/html/unnamed-chunk-3_468cce3889cc3f15181f21cf267b4e9f'}\n\n```{.python .cell-code}\nimport glob\nimport pandas as pd\n\nfiles = glob.glob(\"*.txt\")\n\ndf = pd.concat((pd.read_csv(f, sep='\\t') for f in files))\n\ndf.rename(columns={\"# feature\" : \"feature\"}).filter(items=[\"feature\", \"assembly\"]).groupby([\"assembly\",\"feature\"])[\"feature\"].count()\n```\n:::\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}